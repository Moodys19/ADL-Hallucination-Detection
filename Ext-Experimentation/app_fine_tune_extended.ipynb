{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Subproject - Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW  # Import PyTorch's AdamW\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification , AdamW, BertTokenizerFast\n",
    "#from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv(\"cnndm/train_data_ext.csv\", sep=';')\n",
    "valid_data = pd.read_csv(\"cnndm/valid_data_ext.csv\", sep=';')\n",
    "test_data = pd.read_csv(\"cnndm/test_data_ext.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "article       0\n",
      "highlights    0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
       "      <td>Five small polyps found during procedure; \"non...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>(CNN)  -- The National Football League has ind...</td>\n",
       "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            article  \\\n",
       "0           1  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1           2  Editor's note: In our Behind the Scenes series...   \n",
       "2           3  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
       "3           4  WASHINGTON (CNN) -- Doctors removed five small...   \n",
       "4           5  (CNN)  -- The National Football League has ind...   \n",
       "\n",
       "                                          highlights  label  \n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...      0  \n",
       "1  Mentally ill inmates in Miami are housed on th...      0  \n",
       "2  NEW: \"I thought I was going to die,\" driver sa...      0  \n",
       "3  Five small polyps found during procedure; \"non...      0  \n",
       "4  NEW: NFL chief, Atlanta Falcons owner critical...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.isna().sum())\n",
    "missing_rows = train_data[train_data.isna().any(axis=1)]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13789</th>\n",
       "      <td>69951</td>\n",
       "      <td>ATLANTA, Georgia (CNN) -- Shoes tell a lot abo...</td>\n",
       "      <td>Some people get [B-hallucinated] rare diseases...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13790</th>\n",
       "      <td>69971</td>\n",
       "      <td>ATLANTA, Georgia (CNN) --   An advisory panel ...</td>\n",
       "      <td>Panel recommends health care workers wear [B-h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13791</th>\n",
       "      <td>69981</td>\n",
       "      <td>Washington (CNN) -- President Obama toasted a ...</td>\n",
       "      <td>NEW: [B-hallucinated]Obama unexpectedly breaks...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13792</th>\n",
       "      <td>69991</td>\n",
       "      <td>Editor's note: Tananarive Due is an American B...</td>\n",
       "      <td>Tananarive Due: [B-hallucinated]admits her fam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13793</th>\n",
       "      <td>70001</td>\n",
       "      <td>(CNN) -- Health officials say the H1N1 virus, ...</td>\n",
       "      <td>CDC reported higher levels of flu activity tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            article  \\\n",
       "13789       69951  ATLANTA, Georgia (CNN) -- Shoes tell a lot abo...   \n",
       "13790       69971  ATLANTA, Georgia (CNN) --   An advisory panel ...   \n",
       "13791       69981  Washington (CNN) -- President Obama toasted a ...   \n",
       "13792       69991  Editor's note: Tananarive Due is an American B...   \n",
       "13793       70001  (CNN) -- Health officials say the H1N1 virus, ...   \n",
       "\n",
       "                                              highlights  label  \n",
       "13789  Some people get [B-hallucinated] rare diseases...      1  \n",
       "13790  Panel recommends health care workers wear [B-h...      1  \n",
       "13791  NEW: [B-hallucinated]Obama unexpectedly breaks...      1  \n",
       "13792  Tananarive Due: [B-hallucinated]admits her fam...      1  \n",
       "13793  CDC reported higher levels of flu activity tha...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = BertForTokenClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLevelDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        self.skipped_count = 0  # Counter for rows skipped due to document length\n",
    "        self.skipped_bc_chunk = 0  # Counter for rows skipped due to chunking issues\n",
    "\n",
    "        self._create_examples()\n",
    "\n",
    "    def _create_examples(self):\n",
    "        for _, row in self.data.iterrows():\n",
    "            doc = row['article']\n",
    "            summ = row['highlights']\n",
    "\n",
    "            # Replace [B-hallucinated] and [E-hallucinated] with markers\n",
    "            summ = summ.replace(\"[B-hallucinated]\", \" B_hall \").replace(\"[E-hallucinated]\", \" E_hall \")\n",
    "\n",
    "            # Tokenize the document with offsets\n",
    "            doc_tokenized = self.tokenizer(doc, padding=False, truncation=False)\n",
    "            doc_tokens = doc_tokenized[\"input_ids\"]\n",
    "\n",
    "            # Tokenize the summary with offsets\n",
    "            summ_tokenized = self.tokenizer(\n",
    "                summ, padding=False, truncation=False, return_offsets_mapping=True\n",
    "            )\n",
    "            summ_tokens = summ_tokenized[\"input_ids\"]\n",
    "            summ_offsets = summ_tokenized[\"offset_mapping\"]\n",
    "\n",
    "            if len(doc_tokens) + 3 > self.max_length:  # Check if document alone exceeds the max length\n",
    "                self.skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Helper function to check if a chunk fits within max_length\n",
    "            def chunk_fits(tokens_chunk):\n",
    "                return len(doc_tokens) + len(tokens_chunk) + 3 <= self.max_length\n",
    "\n",
    "            # Case 1: Full summary fits\n",
    "            if chunk_fits(summ_tokens):\n",
    "                self._add_example(doc_tokens, summ_tokens, summ_offsets)\n",
    "            else:\n",
    "                # Case 2: Try splitting into halves\n",
    "                mid = len(summ_tokens) // 2\n",
    "                if chunk_fits(summ_tokens[:mid]) and chunk_fits(summ_tokens[mid:]):\n",
    "                    for chunk, offsets in zip([summ_tokens[:mid], summ_tokens[mid:]],\n",
    "                                            [summ_offsets[:mid], summ_offsets[mid:]]):\n",
    "                        self._add_example(doc_tokens, chunk, offsets)\n",
    "                else:\n",
    "                    # Case 3: Try splitting into thirds\n",
    "                    third = len(summ_tokens) // 3\n",
    "                    chunks = [summ_tokens[:third], summ_tokens[third:2 * third], summ_tokens[2 * third:]]\n",
    "                    offsets_chunks = [\n",
    "                        summ_offsets[:third],\n",
    "                        summ_offsets[third:2 * third],\n",
    "                        summ_offsets[2 * third:]\n",
    "                    ]\n",
    "                    if all(chunk_fits(chunk) for chunk in chunks):\n",
    "                        for chunk, offsets in zip(chunks, offsets_chunks):\n",
    "                            self._add_example(doc_tokens, chunk, offsets)\n",
    "                    else:\n",
    "                        # Case 4: Skip if none of the strategies work\n",
    "                        self.skipped_bc_chunk += 1\n",
    "\n",
    "    def _add_example(self, doc_tokens, summ_tokens, summ_offsets):\n",
    "        input_ids = [self.tokenizer.cls_token_id] + doc_tokens + \\\n",
    "                    [self.tokenizer.sep_token_id] + summ_tokens + \\\n",
    "                    [self.tokenizer.sep_token_id]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Generate token-level labels\n",
    "        token_labels = [0] * len(input_ids)  # Initialize all labels as 0\n",
    "        for idx, (start, end) in enumerate(summ_offsets):\n",
    "            if start == 0 and end == 0:  # Skip special tokens\n",
    "                continue\n",
    "            token_text = self.tokenizer.convert_ids_to_tokens([summ_tokens[idx]])[0]\n",
    "            if token_text in [\"B_hall\", \"E_hall\"]:\n",
    "                token_labels[idx + len(doc_tokens) + 2] = 1  # Adjust index for document and special tokens\n",
    "\n",
    "        # Pad if necessary\n",
    "        if len(input_ids) < self.max_length:\n",
    "            pad_length = self.max_length - len(input_ids)\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_length\n",
    "            attention_mask += [0] * pad_length\n",
    "            token_labels += [-100] * pad_length  # Ignore padded positions in loss computation\n",
    "\n",
    "        self.examples.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_labels\": token_labels\n",
    "        })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(example[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(example[\"attention_mask\"], dtype=torch.long),\n",
    "            \"token_labels\": torch.tensor(example[\"token_labels\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 512 # 2048 # das setzen wir als balance zwischen wie viele padding brauchen und wie viele rausfallen\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TokenLevelDataset(train_data, tokenizer, max_length=MAX_LEN)\n",
    "valid_dataset = TokenLevelDataset(valid_data, tokenizer, max_length=MAX_LEN)\n",
    "test_dataset = TokenLevelDataset(test_data, tokenizer, max_length=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10082/13794 ~ 73.08974916630419 %\n",
      "Test: 1350/1974 ~ 68.38905775075987 %\n",
      "Valid: 1412/1966 ~ 71.82095625635809 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {train_dataset.skipped_count}/{train_data.shape[0]} ~ {train_dataset.skipped_count/train_data.shape[0] * 100} %')\n",
    "print(f'Test: {test_dataset.skipped_count}/{test_data.shape[0]} ~ {test_dataset.skipped_count/test_data.shape[0] * 100} %')\n",
    "print(f'Valid: {valid_dataset.skipped_count}/{valid_data.shape[0]} ~ {valid_dataset.skipped_count/valid_data.shape[0] * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 383/13794 ~ 2.7765695229810063 %\n",
      "Test: 42/1974 ~ 2.127659574468085 %\n",
      "Valid: 40/1966 ~ 2.034587995930824 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {train_dataset.skipped_bc_chunk}/{train_data.shape[0]} ~ {train_dataset.skipped_bc_chunk/train_data.shape[0] * 100} %')\n",
    "print(f'Test: {test_dataset.skipped_bc_chunk}/{test_data.shape[0]} ~ {test_dataset.skipped_bc_chunk/test_data.shape[0] * 100} %')\n",
    "print(f'Valid: {valid_dataset.skipped_bc_chunk}/{valid_data.shape[0]} ~ {valid_dataset.skipped_bc_chunk/valid_data.shape[0] * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mocca\\anaconda3\\envs\\adl_project\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_labels = batch['token_labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "        # Flatten logits and labels for loss computation\n",
    "        logits = logits.view(-1, logits.shape[-1])  # Shape: (batch_size * seq_len, num_labels)\n",
    "        labels = token_labels.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "        # Debugging: print shapes\n",
    "        #print(f\"Logits shape: {logits.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Predictions\n",
    "        preds = torch.argmax(logits, dim=1)  # Shape: (batch_size * seq_len)\n",
    "        mask = labels != -100  # Ignore padded labels\n",
    "        correct_predictions += torch.sum((preds == labels) & mask)\n",
    "        total_tokens += torch.sum(mask)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if total_tokens == 0:\n",
    "        raise ValueError(\"Total tokens processed is zero. Check your token labels or dataset.\")\n",
    "\n",
    "    accuracy = correct_predictions.double() / total_tokens\n",
    "    return accuracy, sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_labels = batch['token_labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "            # Flatten logits and labels for loss computation\n",
    "            logits = logits.view(-1, logits.shape[-1])  # Shape: (batch_size * seq_len, num_labels)\n",
    "            labels = token_labels.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Predictions\n",
    "            preds = torch.argmax(logits, dim=1)  # Shape: (batch_size * seq_len)\n",
    "            mask = labels != -100  # Ignore padded labels\n",
    "            correct_predictions += torch.sum((preds == labels) & mask)\n",
    "            total_tokens += torch.sum(mask)\n",
    "\n",
    "            # Collect predictions and labels for evaluation metrics\n",
    "            all_preds.extend(preds[mask].cpu().numpy())\n",
    "            all_labels.extend(labels[mask].cpu().numpy())\n",
    "\n",
    "    accuracy = correct_predictions.double() / total_tokens\n",
    "\n",
    "    return accuracy, sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n",
      "Train Loss: 0.0057, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0038, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 2/20\n",
      "----------\n",
      "Train Loss: 0.0043, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0029, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 3/20\n",
      "----------\n",
      "Train Loss: 0.0034, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0023, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 4/20\n",
      "----------\n",
      "Train Loss: 0.0026, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0018, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 5/20\n",
      "----------\n",
      "Train Loss: 0.0021, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0014, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 6/20\n",
      "----------\n",
      "Train Loss: 0.0017, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0012, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 7/20\n",
      "----------\n",
      "Train Loss: 0.0014, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0009, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 8/20\n",
      "----------\n",
      "Train Loss: 0.0012, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0008, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 9/20\n",
      "----------\n",
      "Train Loss: 0.0010, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0006, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 10/20\n",
      "----------\n",
      "Train Loss: 0.0008, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0005, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 11/20\n",
      "----------\n",
      "Train Loss: 0.0007, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0004, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 12/20\n",
      "----------\n",
      "Train Loss: 0.0006, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0004, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 13/20\n",
      "----------\n",
      "Train Loss: 0.0005, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0003, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 14/20\n",
      "----------\n",
      "Train Loss: 0.0004, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0003, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 15/20\n",
      "----------\n",
      "Train Loss: 0.0003, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0002, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 16/20\n",
      "----------\n",
      "Train Loss: 0.0003, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0002, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 17/20\n",
      "----------\n",
      "Train Loss: 0.0003, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0002, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 18/20\n",
      "----------\n",
      "Train Loss: 0.0002, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0001, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 19/20\n",
      "----------\n",
      "Train Loss: 0.0002, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0001, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n",
      "Epoch 20/20\n",
      "----------\n",
      "Train Loss: 0.0002, Train Token Accuracy: 1.0000\n",
      "Validation Loss: 0.0001, Validation Token Accuracy: 1.0000\n",
      "Validation performance improved. Model saved.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20  # Maximum number of epochs\n",
    "PATIENCE = 5  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')  # Initialize with a large value\n",
    "early_stopping_counter = 0  # Tracks epochs without improvement\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Token Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_acc, val_loss = eval_model(model, valid_loader, criterion, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Token Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0  # Reset counter if performance improves\n",
    "        torch.save(model.state_dict(), 'models/best_model_state_token.bin')  # Save the best model\n",
    "        print(\"Validation performance improved. Model saved.\")\n",
    "    else:\n",
    "        early_stopping_counter += 1  # Increment counter if no improvement\n",
    "        print(f\"No improvement. Early stopping counter: {early_stopping_counter}/{PATIENCE}\")\n",
    "\n",
    "    # Stop training if early stopping criteria are met\n",
    "    if early_stopping_counter >= PATIENCE:\n",
    "        print(\"Early stopping triggered. Training stopped.\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
