{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extended Subproject - Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW  # Import PyTorch's AdamW\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv(\"cnndm/train_data_ext.csv\", sep=';')\n",
    "valid_data = pd.read_csv(\"cnndm/valid_data_ext.csv\", sep=';')\n",
    "test_data = pd.read_csv(\"cnndm/test_data_ext.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isna().sum())\n",
    "missing_rows = train_data[train_data.isna().any(axis=1)]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "# Load Longformer model for classification\n",
    "model = LongformerForSequenceClassification.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "\n",
    "# # Check model details\n",
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenLevelDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=2048):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        self.skipped_count = 0  # Counter for rows skipped due to document length\n",
    "        self.skipped_bc_chunk = 0  # Counter for rows skipped due to chunking issues\n",
    "\n",
    "        self._create_examples()\n",
    "\n",
    "    def _create_examples(self):\n",
    "        for _, row in self.data.iterrows():\n",
    "            doc, summ, hallucination_labels = row['article'], row['highlights'], row['labels']\n",
    "\n",
    "            # Tokenize document and summary\n",
    "            doc_tokens = self.tokenizer.tokenize(doc)\n",
    "            summ_tokens = self.tokenizer.tokenize(summ)\n",
    "\n",
    "            # Ensure document fits within max_length alone\n",
    "            if len(doc_tokens) + 3 > self.max_length:  # [CLS] doc_tokens [SEP]\n",
    "                self.skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Function to check if a chunk fits within max_length\n",
    "            def chunk_fits(tokens_chunk):\n",
    "                return len(doc_tokens) + len(tokens_chunk) + 3 <= self.max_length\n",
    "\n",
    "            # Case 1: Check if the full summary fits\n",
    "            if chunk_fits(summ_tokens):\n",
    "                input_ids, attention_mask, labels = self._create_input(doc_tokens, summ_tokens, hallucination_labels)\n",
    "                self.examples.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels})\n",
    "            else:\n",
    "                # Case 2: Try splitting into halves\n",
    "                mid = len(summ_tokens) // 2\n",
    "                if chunk_fits(summ_tokens[:mid]) and chunk_fits(summ_tokens[mid:]):\n",
    "                    for chunk, chunk_labels in zip(\n",
    "                        [summ_tokens[:mid], summ_tokens[mid:]],\n",
    "                        [hallucination_labels[:mid], hallucination_labels[mid:]]\n",
    "                    ):\n",
    "                        input_ids, attention_mask, labels = self._create_input(doc_tokens, chunk, chunk_labels)\n",
    "                        self.examples.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels})\n",
    "                else:\n",
    "                    # Case 3: Try splitting into thirds\n",
    "                    third = len(summ_tokens) // 3\n",
    "                    chunks = [summ_tokens[:third], summ_tokens[third:2 * third], summ_tokens[2 * third:]]\n",
    "                    chunk_labels = [\n",
    "                        hallucination_labels[:third],\n",
    "                        hallucination_labels[third:2 * third],\n",
    "                        hallucination_labels[2 * third:]\n",
    "                    ]\n",
    "                    if all(chunk_fits(chunk) for chunk in chunks):\n",
    "                        for chunk, chunk_label in zip(chunks, chunk_labels):\n",
    "                            input_ids, attention_mask, labels = self._create_input(doc_tokens, chunk, chunk_label)\n",
    "                            self.examples.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels})\n",
    "                    else:\n",
    "                        # Case 4: Skip if none of the strategies work\n",
    "                        self.skipped_bc_chunk += 1\n",
    "\n",
    "    def _create_input(self, doc_tokens, summ_tokens, hallucination_labels):\n",
    "        input_ids = [self.tokenizer.cls_token_id] + \\\n",
    "                    self.tokenizer.convert_tokens_to_ids(doc_tokens) + \\\n",
    "                    [self.tokenizer.sep_token_id] + \\\n",
    "                    self.tokenizer.convert_tokens_to_ids(summ_tokens) + \\\n",
    "                    [self.tokenizer.sep_token_id]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Align labels\n",
    "        labels = [-100] * len(input_ids)  # Initialize with ignored index\n",
    "        doc_len = len(doc_tokens)\n",
    "        labels[doc_len + 2:doc_len + 2 + len(hallucination_labels)] = hallucination_labels\n",
    "\n",
    "        # Pad if necessary\n",
    "        if len(input_ids) < self.max_length:\n",
    "            pad_length = self.max_length - len(input_ids)\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_length\n",
    "            attention_mask += [0] * pad_length\n",
    "            labels += [-100] * pad_length  # Ignore padding tokens\n",
    "\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(example[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(example[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(example[\"labels\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_LEN = 2048 # das setzen wir als balance zwischen wie viele padding brauchen und wie viele rausfallen\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = TokenLevelDataset(train_data, tokenizer, max_length=MAX_LEN)\n",
    "valid_dataset = TokenLevelDataset(valid_data, tokenizer, max_length=MAX_LEN)\n",
    "test_dataset = TokenLevelDataset(test_data, tokenizer, max_length=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train: {train_dataset.skipped_count}/{train_data.shape[0]} ~ {train_dataset.skipped_count/train_data.shape[0] * 100} %')\n",
    "print(f'Test: {test_dataset.skipped_count}/{test_data.shape[0]} ~ {test_dataset.skipped_count/test_data.shape[0] * 100} %')\n",
    "print(f'Valid: {valid_dataset.skipped_count}/{valid_data.shape[0]} ~ {valid_dataset.skipped_count/valid_data.shape[0] * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train: {train_dataset.skipped_bc_chunk}/{train_data.shape[0]} ~ {train_dataset.skipped_bc_chunk/train_data.shape[0] * 100} %')\n",
    "print(f'Test: {test_dataset.skipped_bc_chunk}/{test_data.shape[0]} ~ {test_dataset.skipped_bc_chunk/test_data.shape[0] * 100} %')\n",
    "print(f'Valid: {valid_dataset.skipped_bc_chunk}/{valid_data.shape[0]} ~ {valid_dataset.skipped_bc_chunk/valid_data.shape[0] * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
