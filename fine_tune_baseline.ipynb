{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "74as8no9Hgj1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.optim import AdamW  # Import PyTorch's AdamW\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "#from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8aq3yKnHpxf",
        "outputId": "ffbf2855-3336-41af-d2d9-fd62588faeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/ADL-Hallucination-Detection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9sML1N_vIYSo"
      },
      "outputs": [],
      "source": [
        "# Load the datasets\n",
        "train_data = pd.read_csv(\"cnndm/train_data_base.csv\", sep=';')\n",
        "valid_data = pd.read_csv(\"cnndm/valid_data_base.csv\", sep=';')\n",
        "test_data = pd.read_csv(\"cnndm/test_data_base.csv\", sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeNIHNqQN1C2",
        "outputId": "95725fda-828f-4055-a621-482d58c4a76f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unnamed: 0    0\n",
            "article       0\n",
            "highlights    0\n",
            "label         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train_data.isna().sum())\n",
        "missing_rows = train_data[train_data.isna().any(axis=1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "L3xt017hLKd2",
        "outputId": "a0b8e83f-2500-47d0-f0f6-b875aa723cbe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
              "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
              "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n",
              "      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n",
              "      <td>Five small polyps found during procedure; \"non...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>(CNN)  -- The National Football League has ind...</td>\n",
              "      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                            article  \\\n",
              "0           1  LONDON, England (Reuters) -- Harry Potter star...   \n",
              "1           2  Editor's note: In our Behind the Scenes series...   \n",
              "2           3  MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n",
              "3           4  WASHINGTON (CNN) -- Doctors removed five small...   \n",
              "4           5  (CNN)  -- The National Football League has ind...   \n",
              "\n",
              "                                          highlights  label  \n",
              "0  Harry Potter star Daniel Radcliffe gets £20M f...      0  \n",
              "1  Mentally ill inmates in Miami are housed on th...      0  \n",
              "2  NEW: \"I thought I was going to die,\" driver sa...      0  \n",
              "3  Five small polyps found during procedure; \"non...      0  \n",
              "4  NEW: NFL chief, Atlanta Falcons owner critical...      0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Load Longformer model for classification\n",
        "model = LongformerForSequenceClassification.from_pretrained(\n",
        "    \"allenai/longformer-base-4096\",\n",
        "    num_labels=2  # Binary classification\n",
        ")\n",
        "\n",
        "# Check model details\n",
        "print(model.config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCgUdaB4If6w",
        "outputId": "a946f6d6-b564-4e0c-cfa7-7957f3b8d474"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "280f894e7d16490b821d3f8c23208111",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mabsa\\anaconda3\\envs\\adl_project\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mabsa\\.cache\\huggingface\\hub\\models--prajjwal1--bert-tiny. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c2824b7450e4796ab5d1b3593b012c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7581a9c66cd543fd8b67df41afbffebc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# # Load the tokenizer and model\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
        "# model = BertForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Document Lengths  Summary Lengths  Combined Lengths\n",
            "count      13952.000000     13952.000000      13952.000000\n",
            "mean         806.338159        81.510464        890.848624\n",
            "std          400.513897        60.774140        409.056411\n",
            "min           27.000000         8.000000         70.000000\n",
            "25%          488.000000        54.000000        571.000000\n",
            "50%          747.000000        65.000000        831.000000\n",
            "75%         1069.250000        87.000000       1153.000000\n",
            "max         2496.000000       642.000000       2593.000000\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and calculate token lengths\n",
        "def calculate_lengths(data, tokenizer, column_name):\n",
        "    lengths = []\n",
        "    for text in data[column_name]:\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        lengths.append(len(tokens))\n",
        "    return lengths\n",
        "\n",
        "# Calculate token lengths for documents and summaries\n",
        "doc_lengths = calculate_lengths(train_data, tokenizer, column_name='article')\n",
        "summ_lengths = calculate_lengths(train_data, tokenizer, column_name='highlights')\n",
        "\n",
        "# Combine lengths for total sequence lengths\n",
        "combined_lengths = [d + s + 3 for d, s in zip(doc_lengths, summ_lengths)]  # +3 for [CLS], [SEP], [SEP]\n",
        "\n",
        "# Create a DataFrame for analysis\n",
        "lengths_df = pd.DataFrame({\n",
        "    'Document Lengths': doc_lengths,\n",
        "    'Summary Lengths': summ_lengths,\n",
        "    'Combined Lengths': combined_lengths\n",
        "})\n",
        "\n",
        "# Plot distributions\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(lengths_df['Combined Lengths'], bins=50, alpha=0.7, label='Combined Lengths')\n",
        "plt.hist(lengths_df['Document Lengths'], bins=50, alpha=0.7, label='Document Lengths')\n",
        "plt.hist(lengths_df['Summary Lengths'], bins=50, alpha=0.7, label='Summary Lengths')\n",
        "#plt.axvline(x=512, color='red', linestyle='--', label='Max Length (512)')\n",
        "plt.xlabel('Number of Tokens - Training')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.title('Token Length Distributions')\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(lengths_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Document Lengths  Summary Lengths  Combined Lengths\n",
            "count       1986.000000      1986.000000       1986.000000\n",
            "mean         806.749245        65.001511        874.750755\n",
            "std          448.546224        52.270875        458.887068\n",
            "min          104.000000        14.000000        127.000000\n",
            "25%          441.000000        39.000000        505.750000\n",
            "50%          732.000000        52.000000        798.000000\n",
            "75%         1090.000000        72.000000       1164.750000\n",
            "max         2965.000000       519.000000       3050.000000\n"
          ]
        }
      ],
      "source": [
        "# Calculate token lengths for documents and summaries\n",
        "doc_lengths_test = calculate_lengths(test_data, tokenizer, column_name='article')\n",
        "summ_lengths_test = calculate_lengths(test_data, tokenizer, column_name='highlights')\n",
        "\n",
        "# Combine lengths for total sequence lengths\n",
        "combined_lengths_test = [d + s + 3 for d, s in zip(doc_lengths_test, summ_lengths_test)]  # +3 for [CLS], [SEP], [SEP]\n",
        "\n",
        "# Create a DataFrame for analysis\n",
        "lengths_df_test = pd.DataFrame({\n",
        "    'Document Lengths': doc_lengths_test,\n",
        "    'Summary Lengths': summ_lengths_test,\n",
        "    'Combined Lengths': combined_lengths_test\n",
        "})\n",
        "\n",
        "print(lengths_df_test.describe())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Document Lengths  Summary Lengths  Combined Lengths\n",
            "count       1992.000000      1992.000000       1992.000000\n",
            "mean         828.542169        65.669177        897.211345\n",
            "std          444.207719        56.287804        454.320465\n",
            "min          114.000000        11.000000        133.000000\n",
            "25%          474.000000        38.000000        532.750000\n",
            "50%          760.500000        52.000000        826.000000\n",
            "75%         1107.250000        71.000000       1182.250000\n",
            "max         2287.000000       548.000000       2648.000000\n"
          ]
        }
      ],
      "source": [
        "# Calculate token lengths for documents and summaries\n",
        "doc_lengths_valid = calculate_lengths(valid_data, tokenizer, column_name='article')\n",
        "summ_lengths_valid = calculate_lengths(valid_data, tokenizer, column_name='highlights')\n",
        "\n",
        "# Combine lengths for total sequence lengths\n",
        "combined_lengths_valid = [d + s + 3 for d, s in zip(doc_lengths_valid, summ_lengths_valid)]  # +3 for [CLS], [SEP], [SEP]\n",
        "\n",
        "# Create a DataFrame for analysis\n",
        "lengths_df_valid = pd.DataFrame({\n",
        "    'Document Lengths': doc_lengths_valid,\n",
        "    'Summary Lengths': summ_lengths_valid,\n",
        "    'Combined Lengths': combined_lengths_valid\n",
        "})\n",
        "\n",
        "print(lengths_df_valid.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3908\n",
            "588\n",
            "594\n",
            "\n",
            "90\n",
            "24\n",
            "18\n"
          ]
        }
      ],
      "source": [
        "print((lengths_df[\"Document Lengths\"] > 1024).sum())\n",
        "print((lengths_df_test[\"Document Lengths\"] > 1024).sum())\n",
        "print((lengths_df_valid[\"Document Lengths\"] > 1024).sum())\n",
        "print()\n",
        "\n",
        "print((lengths_df[\"Document Lengths\"] > 2048).sum())\n",
        "print((lengths_df_test[\"Document Lengths\"] > 2048).sum())\n",
        "print((lengths_df_valid[\"Document Lengths\"] > 2048).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4716\n",
            "682\n",
            "700\n",
            "\n",
            "157\n",
            "33\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "print((lengths_df[\"Combined Lengths\"] > 1024).sum())\n",
        "print((lengths_df_test[\"Combined Lengths\"] > 1024).sum())\n",
        "print((lengths_df_valid[\"Combined Lengths\"] > 1024).sum())\n",
        "print()\n",
        "\n",
        "print((lengths_df[\"Combined Lengths\"] > 2048).sum())\n",
        "print((lengths_df_test[\"Combined Lengths\"] > 2048).sum())\n",
        "print((lengths_df_valid[\"Combined Lengths\"] > 2048).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "T4K4f2MBKa4z"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.examples = []\n",
        "        self.skipped_count = 0  # Counter for rows skipped due to document length\n",
        "        self.skipped_bc_chunk = 0  # Counter for rows skipped due to chunking issues\n",
        "\n",
        "        self._create_examples()\n",
        "\n",
        "    def _create_examples(self):\n",
        "        for _, row in self.data.iterrows():\n",
        "            doc, summ, label = row['article'], row['highlights'], row['label']\n",
        "\n",
        "            # Tokenize document and summary\n",
        "            doc_tokens = self.tokenizer.tokenize(doc)\n",
        "            summ_tokens = self.tokenizer.tokenize(summ)\n",
        "\n",
        "            # Ensure document fits within max_length alone\n",
        "            if len(doc_tokens) + 3 > self.max_length:  # [CLS] doc_tokens [SEP]\n",
        "                self.skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Function to check if a chunk fits within max_length\n",
        "            def chunk_fits(tokens_chunk):\n",
        "                return len(doc_tokens) + len(tokens_chunk) + 3 <= self.max_length\n",
        "\n",
        "            # Case 1: Check if the full summary fits\n",
        "            if chunk_fits(summ_tokens):\n",
        "                input_ids, attention_mask = self._create_input(doc_tokens, summ_tokens)\n",
        "                self.examples.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label})\n",
        "            else:\n",
        "                # Case 2: Try splitting into halves\n",
        "                mid = len(summ_tokens) // 2\n",
        "                if chunk_fits(summ_tokens[:mid]) and chunk_fits(summ_tokens[mid:]):\n",
        "                    for chunk in [summ_tokens[:mid], summ_tokens[mid:]]:\n",
        "                        input_ids, attention_mask = self._create_input(doc_tokens, chunk)\n",
        "                        self.examples.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label})\n",
        "                else:\n",
        "                    # Case 3: Try splitting into thirds\n",
        "                    third = len(summ_tokens) // 3\n",
        "                    chunks = [summ_tokens[:third], summ_tokens[third:2 * third], summ_tokens[2 * third:]]\n",
        "                    if all(chunk_fits(chunk) for chunk in chunks):\n",
        "                        for chunk in chunks:\n",
        "                            input_ids, attention_mask = self._create_input(doc_tokens, chunk)\n",
        "                            self.examples.append({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label})\n",
        "                    else:\n",
        "                        # Case 4: Skip if none of the strategies work\n",
        "                        self.skipped_bc_chunk += 1\n",
        "\n",
        "    def _create_input(self, doc_tokens, summ_tokens):\n",
        "        input_ids = [self.tokenizer.cls_token_id] + \\\n",
        "                    self.tokenizer.convert_tokens_to_ids(doc_tokens) + \\\n",
        "                    [self.tokenizer.sep_token_id] + \\\n",
        "                    self.tokenizer.convert_tokens_to_ids(summ_tokens) + \\\n",
        "                    [self.tokenizer.sep_token_id]\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad if necessary\n",
        "        if len(input_ids) < self.max_length:\n",
        "            pad_length = self.max_length - len(input_ids)\n",
        "            input_ids += [self.tokenizer.pad_token_id] * pad_length\n",
        "            attention_mask += [0] * pad_length\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(example[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(example[\"attention_mask\"], dtype=torch.long),\n",
        "            \"label\": torch.tensor(example[\"label\"], dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wir schauen uns mal wie viele rausfliegen und wie die token lengths verteilt sind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMcY3BYQKfXz"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "MAX_LEN = 2048 # das setzen wir als balance zwischen wie viele padding brauchen und wie viele rausfallen\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = CustomDataset(train_data, tokenizer, max_length=MAX_LEN)\n",
        "valid_dataset = CustomDataset(valid_data, tokenizer, max_length=MAX_LEN)\n",
        "test_dataset = CustomDataset(test_data, tokenizer, max_length=MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 10196/13952 ~ 73.07912844036697 %\n",
            "Test: 1356/1986 ~ 68.27794561933534 %\n",
            "Valid: 1428/1992 ~ 71.6867469879518 %\n"
          ]
        }
      ],
      "source": [
        "print(f'Train: {train_dataset.skipped_count}/{train_data.shape[0]} ~ {train_dataset.skipped_count/train_data.shape[0] * 100} %')\n",
        "print(f'Test: {test_dataset.skipped_count}/{test_data.shape[0]} ~ {test_dataset.skipped_count/test_data.shape[0] * 100} %')\n",
        "print(f'Valid: {valid_dataset.skipped_count}/{valid_data.shape[0]} ~ {valid_dataset.skipped_count/valid_data.shape[0] * 100} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Train: {train_dataset.skipped_bc_chunk}/{train_data.shape[0]} ~ {train_dataset.skipped_bc_chunk/train_data.shape[0] * 100} %')\n",
        "print(f'Test: {test_dataset.skipped_bc_chunk}/{test_data.shape[0]} ~ {test_dataset.skipped_bc_chunk/test_data.shape[0] * 100} %')\n",
        "print(f'Valid: {valid_dataset.skipped_bc_chunk}/{valid_data.shape[0]} ~ {valid_dataset.skipped_bc_chunk/valid_data.shape[0] * 100} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I1Q2a3xKoy_",
        "outputId": "4ec81ca4-6737-41fb-a8fa-180b55cc5be7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), sum(losses) / len(losses) # TODO hier vielleicht AUC?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "cfJBpJqNKwSW"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "    return correct_predictions.double() / len(data_loader.dataset), sum(losses) / len(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "xT1OHMDLK0A-",
        "outputId": "68519623-6553-430b-c9f0-975fc2b764dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "----------\n",
            "Train loss: 0.6129013518534744 | Train accuracy: 0.6867244829886591\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-a9805d5d71fa>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train loss: {train_loss} | Train accuracy: {train_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Validation loss: {val_loss} | Validation accuracy: {val_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-2891cae20b9f>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, data_loader, criterion, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1669\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mnum_args_in_forward_chunk_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_args_in_forward_chunk_fn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3254\u001b[0;31m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n\u001b[0m\u001b[1;32m   3255\u001b[0m                                    globals=globals, locals=locals, eval_str=eval_str)\n\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3000\u001b[0m                       follow_wrapped=True, globals=None, locals=None, eval_str=False):\n\u001b[1;32m   3001\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0m\u001b[1;32m   3003\u001b[0m                                         \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0;31m# In this case we skip the first parameter of the underlying\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0;31m# function (usually `self` or `cls`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2401\u001b[0;31m         \u001b[0msig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_signature_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2461\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m         return _signature_from_function(sigcls, obj,\n\u001b[0m\u001b[1;32m   2464\u001b[0m                                         \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2368\u001b[0m     \u001b[0;31m# Is 'func' is a pure Python function - don't validate the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[0;31m# parameters list (for correct order and defaults), it should be OK.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m     return cls(parameters,\n\u001b[0m\u001b[1;32m   2371\u001b[0m                \u001b[0mreturn_annotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                __validate_parameters__=is_duck_function)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[1;32m   2967\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMappingProxyType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2967\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMappingProxyType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mname\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2685\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_annotation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2687\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCHS = 50  # Maximum number of epochs\n",
        "PATIENCE = 5  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "early_stopping_counter = 0  # Tracks epochs without improvement\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_acc, train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_acc, val_loss = eval_model(model, valid_loader, criterion, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        early_stopping_counter = 0  # Reset counter if performance improves\n",
        "        torch.save(model.state_dict(), 'best_model_state.bin')  # Save the best model\n",
        "        print(\"Validation performance improved. Model saved.\")\n",
        "    else:\n",
        "        early_stopping_counter += 1  # Increment counter if no improvement\n",
        "        print(f\"No improvement. Early stopping counter: {early_stopping_counter}/{PATIENCE}\")\n",
        "\n",
        "    # Stop training if early stopping criteria are met\n",
        "    if early_stopping_counter >= PATIENCE:\n",
        "        print(\"Early stopping triggered. Training stopped.\")\n",
        "        break\n",
        "\n",
        "# Load the best model after training\n",
        "model.load_state_dict(torch.load('best_model_state.bin'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfBiqoTqK5Ag"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_model_state.bin'))\n",
        "\n",
        "test_acc, test_loss = eval_model(model, test_loader, criterion, device)\n",
        "print(f'Test loss: {test_loss} | Test accuracy: {test_acc}')\n",
        "\n",
        "# Classification report\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "adl_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
