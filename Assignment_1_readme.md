# TODOs

- Im README die folder structure außerhalb des github ordners definieren

---
# Assignment 1 - Initiate: Hallucination Detection in LLM Summaries

##  Project Choice & Motivation
The project I chose aims to develop a model capable of identifying and flagging hallucinations in summaries generated by large language models (LLMs). 
Large Language Models are increasingly becoming a part of our daily lives and are being deployed in high-stakes domains such as medicine, journalism, and legal services, where the reliability of their outputs is crucial. Despite their capabilities, LLMs are prone to hallucinations — i.e. content that is nonsensical or unfaithful to the provided source. Originally coined to describe non-coherent outputs, the term has evolved to encompass nonfactual or arbitrary content generated by LLMs. Even highly capable models like GPT-4 are prone to hallucinations.[1] While hallucinations may be harmless in casual contexts, they pose significant risks when factuality is critical, as in medical or legal environments. Furthermore, the average everyday user is more likely to blindly trust a summary or information provided by an LLM, rather than going the extra mile and fact-checking the information. Therefore, detecting and mitigating hallucinations in LLM-generated content has become an important challenge.

## Approach

### Data Preparation
For the project, I intend to use the [CNN/Daily Mail (CNNDM) dataset](https://huggingface.co/datasets/RUCAIBox/Summarization/blob/main/cnndm.tgz), which contains full-length news articles and corresponding summaries. This dataset provides the foundation for training and evaluating the model. For each article, I intend to generate hallucinated summaries using a pre-trained LLM, such as [LLAMA 3.2](https://www.llama.com/) or [Gemini Nano](https://ai.google.dev/gemini-api/docs?hl=de), by introducing factual inconsistencies or arbitrary content. I also intend to task the LLM to label the created summaries at the token level: tokens that remain factual will be labeled as `O`, while hallucinated tokens will be labeled as `B-HALLUCINATED` (beginning of a hallucinated phrase) and `I-HALLUCINATED` (inside a hallucinated phrase). Of course, creating these labels using the LLM is not optimal due to potential mislabelings, hallucinations, and general errors; but manual labeling would go beyond the time frame planned for this project. In order to mitigate the risks mentioned, I intend to manually check a sample of the created token-labelings and their corresponding hallucinated summaries. The final decision on which LLM I will use depends on resource constraints as well as initial experiments.

In the case that using the CNN/Daily Mail dataset is not feasible, I intend to use the [XSUM dataset](https://huggingface.co/datasets/RUCAIBox/Summarization/tree/main), which consists of BBC articles paired with single-sentence summaries. The shorter summaries could potentially reduce training time and provide a better overview during the data preparation step, helping to assess whether the LLM creates reasonably hallucinated summaries and accurate token labelings. However, a potential downside of using shorter summaries is the risk of oversimplifying the task, which may cause the model to underperform when handling more complex and longer summaries. For this reason, I intend to keep the XSUM dataset as a fallback option, only to be used if, as mentioned, working with the CNN/Daily Mail dataset proves infeasible.


### Model Implementation
Once the data is prepared, I intend to fine-tune a [BERT](https://huggingface.co/docs/transformers/model_doc/bert)[2] model to perform token-level classification, learning to detect hallucinated tokens in the generated summaries.
While this approach is effective for detecting hallucinations at the token level, there are potential shortcomings. Token-level labeling may lead to surface-level detection, meaning that the model may not always capture semantic hallucinations that span across sentences or paragraphs. If the time frame permits, I intend to mitigate these shortcomings by integrating Semantic Entropy Probes (SEPs)[1] into the model. SEPs are linear probes that capture semantic uncertainty from the hidden states of LLMs. Similarly, I plan to use the hidden states of the BERT model to estimate semantic entropy, allowing the model to assess how confident it is in its classification of the token labels. This approach could provide an additional confidence score for the detected hallucinations, potentially helping to flag uncertain and potentially unreliable sections of the summary.

Finally, the model will return a confidence score (likely derived from the maximum of the softmax values for the hallucination token labels) indicating whether a summary contains hallucinations, while also highlighting the hallucinated tokens. If the scope of the project permits, I may add a classification layer to predict the overall probability that a summary contains hallucinations. For this, I would need to label each article summary as either containing a hallucination (1) or not (0). This overall probability could then be used as a general confidence score, providing a broader assessment of the likelihood of hallucination in the entire summary.

### Application
The final output will include an API that accepts a full article and summary as input, highlights hallucinated sections, and returns a confidence score. If time permits, I may extend the API such that it takes a full text as an input and directly generates a summary for which it will run the hallucination detection.

## Project Type
Intuitively, my project falls into the "Bring Your Own Method" category, as I intend to fine-tune an existing model architecture (the BERT model) and extend it with the SEPs discussed in the model implementation section. A weaker case could be made that the first part of the project aligns with the "Bring Your Own Dataset" category, as I plan to append or extend an existing dataset with hallucinated summaries.

## Related Work and Sources
The following papers were considered for my project idea:

[1] Jannik Kossen, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, Yarin Gal, Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs, 2023.

[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019.

[3] Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, Bill Dolan, A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation, 2022.

[4] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal, Detecting hallucinations in large language models using semantic entropy, 2023.

## Work Breakdown
**Disclaimer**: This timetable is purely theoretical, as I have limited experience with a project of this scope and therefore cannot provide a fully reliable time estimate. Some tasks may take less time, while others may require significantly more.

- Dataset preparation *4-8 hours*
    - Decide on and get access for LLM
    - Prepare hallucinated summaries
    - Label the tokens for hallucination detection
    - Manual control of sample

- Implementation of BERT model architecture (+ integrationg SEPs) *15 to 20 hours + 5 to 15 hours (due to lack of experience)*
    - Setup the Model
    - Data preparation (data loading, tokenization etc.) 
    - token-level classification setup
    - Fine-tuning setup
    - (Implement SEPs for semantic entropy calculation)
    - (Implement classification head)

- Running training, fine-tuning experiments and evaluations *20 hours*
    - Experimenting with parameter tuning
    - Evaluations of the token classification results and analyzing the impact of SEPs on detecting hallucinated sections.
    
- Code clean-up and documentation *5 hours*

- API-Creation *5 hours (due to lack of experience)*

- Final Report *5-7 hours*

- Final Presentation preparation *5 hours*
    - Concept/script creation
    - Research on how to film and edit videos
    - Finding courage
    - Film final presentation
    - Potential editing

